{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "TO RUN CODE: GO TO RUNTIME TAB (OR SIMILAR TAB) AND SELECT RUN ALL (OR RESTART AND RUN ALL IF AN ERROR OCCURRED AND WAS FIXED).\n",
        "\n",
        "PLEASE MAKE SURE CSV FILES ARE IN UTF-8 ENCODING OR ELSE ERRORS WILL OCCUR.\n",
        "\n"
      ],
      "metadata": {
        "id": "Tsr4vUVAzT5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# Import libraries\n",
        "import sys\n",
        "import subprocess\n",
        "import os\n",
        "from datetime import datetime as dt\n",
        "\n",
        "try:\n",
        "  import pandas as pd\n",
        "except ImportError or ModuleNotFoundError:\n",
        "  subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'pandas'])\n",
        "  import pandas as pd\n",
        "\n",
        "try:\n",
        "  import numpy as np\n",
        "except ImportError or ModuleNotFoundError:\n",
        "  subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'numpy'])\n",
        "  import numpy as np\n",
        "\n",
        "try:\n",
        "  import xlsxwriter\n",
        "except ImportError or ModuleNotFoundError:\n",
        "  subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'xlsxwriter'])\n",
        "  import xlsxwriter\n",
        "\n",
        "try:\n",
        "  import calweek\n",
        "except ImportError or ModuleNotFoundError:\n",
        "  subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'calweek'])\n",
        "  import calweek\n",
        "\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "Du53MIvinUfz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def csv_import(csv_file: 'UTF-8 csv file') -> 'Pandas dataframe':\n",
        "  '''\n",
        "  Description:\n",
        "    Imports Weekly Tracker CSV file\n",
        "\n",
        "  Arguments:\n",
        "    csv_file: The uploaded csv file\n",
        "\n",
        "  Return:\n",
        "    df: The imported csv as a Pandas dataframe\n",
        "\n",
        "  '''\n",
        "  fname = list(csv_file.keys())[0] # Name of csv file\n",
        "\n",
        "  # Read CSV file as data table\n",
        "  with open(fname, 'r') as f:\n",
        "    try:\n",
        "      df = pd.read_csv(f, header = None, low_memory = False, encoding = 'utf-8') # Import csv as data table\n",
        "    except:\n",
        "      print('Please check to see if csv file name is correct and is in UTF-8 encoding')\n",
        "    else:\n",
        "      print('CSV import successful\\n')\n",
        "\n",
        "  # Remove columns and rows with no entries\n",
        "  df.dropna(axis = 'index', how = 'all', inplace = True)\n",
        "  df.dropna(axis = 'columns', how = 'all', inplace = True)\n",
        "  if 'ï»¿' in df[0].unique():\n",
        "    df.drop(labels = 0, inplace = True, axis = 'columns') # Removes the column containing UTF-8 BOM, which doesn't appear to be an issue in Google Colab\n",
        "\n",
        "  # Dataframe information\n",
        "  # df.head()\n",
        "\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "NJrhkMabc48Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def col_adjust(df_raw: 'Pandas Dataframe', discip_names: list = ['DEVELOPMENT', 'ORE HAULAGE', 'WASTE HAULAGE', 'BACKFILLING', 'REMOTE MUCKING', 'LONGHOLE'], activ_names: list = ['WASTE DEVELOPMENT', 'SILL DEVELOPMENT', 'TRUCKING', 'STOPE', 'BLASTING', 'DRILLING']):\n",
        "  '''\n",
        "    Description:\n",
        "      Adjust the columns by inserting or renaming\n",
        "\n",
        "    Arguments:\n",
        "      df_raw: The raw dataframe\n",
        "      discip_names: The options for discipline\n",
        "      activ_names: The options for activity\n",
        "\n",
        "    Return:\n",
        "      df: The column adjusted dataframe\n",
        "  '''\n",
        "  # Create new columns\n",
        "  df = df_raw.copy()\n",
        "  del df_raw\n",
        "  df.insert(loc = 0, column = 'Discipline', value = np.NaN) # Create a column for discipline and fill with NaN\n",
        "  df.insert(loc = 1, column = 'Activity', value = np.NaN) # Create a column for activity and fill with NaN\n",
        "\n",
        "  # Change names of existing columns\n",
        "  heading_col = df.columns[ df.applymap(lambda word: str(word).lower() in 'heading').any() ][0] # Find the column containing the word Headings\n",
        "  df.rename(columns = {heading_col : 'Location'}, inplace = True) # Rename column containing headings\n",
        "  act_col = df.columns[ df.applymap(lambda word: str(word).lower() in 'actual').any() ][0] # Find the column containing the word Actual\n",
        "  df.rename(columns = {act_col: 'Actual/Planned'}, inplace = True) # Rename column containing planned or actual\n",
        "\n",
        "  # Set the other columns to the corresponding date\n",
        "  row_dates = df[ df.applymap(lambda word: str(word).lower() in 'thursday').any(axis = 'columns') ].index[0] - 1 # The row above the days of the week, which correspond to the dates\n",
        "  df.loc[row_dates] = df.loc[row_dates].fillna(method = 'ffill') # Fill adjacent NaN cells as the dates in the row containing the dates\n",
        "  row_dates = df.loc[row_dates, act_col+1:act_col+15] # The dates in the row containing the dates\n",
        "  orig_date_cols = [col for col in range(act_col+1, act_col+15)] # The original column names that contain the dates\n",
        "  new_date_cols = [ dt.strptime(date, '%d-%b-%y').strftime('%Y-%m-%d') + ' D/S' if orig_date_col % 2 == 0 else dt.strptime(date, '%d-%b-%y').strftime('%Y-%m-%d') + ' N/S' for date, orig_date_col in zip(row_dates, orig_date_cols) ] # The dates in YYYY-MM-DD and indicates day/night shift\n",
        "  # Change the original column names to the dates\n",
        "  df.rename(columns = dict( zip(orig_date_cols, new_date_cols) ), inplace = True)\n",
        "\n",
        "  # Labels for Activity & Discipline\n",
        "  disc = list( set(discip_names) )\n",
        "  act = list( set(activ_names + ['WASTE TO TLO', 'WASTE TO SURFACE']) ) # Waste to TLO & Surface aren't actually in SLR tracker activity category but needed at this point\n",
        "\n",
        "  # Copy the Activity/Discipline to the new columns and remove the original column\n",
        "  orig_col = df.columns[ df.applymap(lambda word: str(word).lower() in 'development').any() ][0] # Column containing the activities/disciplines\n",
        "  df['Activity'] = df['Activity'].where( ~df[orig_col].isin(act), df[orig_col], axis = 'index' ) # Add entries that match the activity list to the activity column\n",
        "  df['Discipline'] = df['Discipline'].where( ~df[orig_col].isin(disc), df[orig_col], axis = 'index' ) # Add entries that match the discipline list to the discipline column\n",
        "  df.drop(labels = orig_col, inplace = True, axis = 'columns') # Remove the original column\n",
        "\n",
        "  # Dataframe information\n",
        "  # df\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "S9JuZ7qloAzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def data_cleaning(df):\n",
        "  '''\n",
        "    Description:\n",
        "      Clean the data table such that it only contains information required for SLR tracker entries\n",
        "\n",
        "    Arguments:\n",
        "      df: The raw dataframe\n",
        "\n",
        "    Outputs:\n",
        "      df: The cleaned dataframe\n",
        "      dates: The dates for the weekly plan\n",
        "\n",
        "  '''\n",
        "  # Remove everything after the last row and column containing useful data\n",
        "  df = df_in.copy()\n",
        "  del df_in\n",
        "  last_row = df[['Actual/Planned']][df[['Actual/Planned']].applymap(lambda word: str(word) in ('Plan' or 'Actual')).any(axis = 'columns')].index[-1] # Get last useful row index\n",
        "  last_col = df.columns.get_loc('Actual/Planned') + 14 # Last useful column\n",
        "  df = df.loc[:last_row, df.columns[:last_col+1]] # Replace existing dataframe removing rows and columns beyond last useful ones\n",
        "\n",
        "  # Append any additional text to Location column from adjacent right column\n",
        "  col_text = df.columns.get_loc('Location') + 1 # The column name to the adjacent right of Location column\n",
        "  rows_text = df[ df[[col_text]].applymap(lambda word: type(word) == str).any(axis = 'columns') ].index # Indices in adjacent column containing string text to be appended\n",
        "  text_to_append = df.loc[rows_text, 'Location'].astype(str) + ' ' + df.loc[rows_text, col_text].astype(str) # New text for Location column\n",
        "  df.loc[rows_text, 'Location'] = text_to_append # Add new text to Location column\n",
        "\n",
        "  # Remove the rest of the useless columns\n",
        "  df.drop(labels = [c for c in range(col_text, df.columns.get_loc('Actual/Planned'))], axis = 'columns', inplace = True, errors = 'ignore')\n",
        "\n",
        "  # Fill every NaN in Activity and Discipline columns based on adjacent entries\n",
        "  df[['Discipline', 'Activity']] = df[['Discipline', 'Activity']].fillna(method = 'ffill')\n",
        "\n",
        "  # Remove NaN entries in Location column\n",
        "  df.dropna(subset = 'Location', inplace = True)\n",
        "\n",
        "  # Remove rows in Location column containing 0's in both numerical and string forms\n",
        "  df = df[ (df['Location'] != '0') & (df['Location'] != 0.0) ]\n",
        "\n",
        "  # Remove rows in Location column containing the words heading, truck, stope, and department\n",
        "  df = df[ ~(df['Location'].str.contains('truck|stope|department|heading', case = False)) ]\n",
        "\n",
        "  # Drop Headings without any activities\n",
        "  dates = list(df.columns)[-14:] # The dates (7 days of week, 2 shifts)\n",
        "  df.dropna(subset = dates, thresh = 1, inplace = True)\n",
        "  df.reset_index(drop = True, inplace = True) # Reset indices\n",
        "\n",
        "  # Correct the entries in Activity column\n",
        "  where_repl = ['ORE HAULAGE', 'BACKFILLING', 'REMOTE MUCKING'] # For which columns to look\n",
        "  what_repl = ['SILL DEVELOPMENT', 'WASTE TO SURFACE', 'WASTE TO SURFACE'] # For what entries to look\n",
        "  repl_with = ['TRUCKING', 'STOPE', 'STOPE'] # 'With what the entries should be replaced\n",
        "  for where, what, word in zip(where_repl, what_repl, repl_with):\n",
        "    mask = df['Discipline'] == where # Filtering to narrow down entries of interest\n",
        "    df[mask] = df[mask].replace(to_replace = what, value = word)\n",
        "\n",
        "  # Fill NaN in Actual/Planned Column with Actual (Corresponding to waste haulage and remote mucking)\n",
        "  df['Actual/Planned'] = df['Actual/Planned'].fillna(value = 'Actual')\n",
        "\n",
        "  # Dataframe information\n",
        "  # df[dates].count().sum() # Check total number of non zero entries to check with excel file\n",
        "  # df\n",
        "\n",
        "  return df, dates"
      ],
      "metadata": {
        "id": "gl5DnCFChTMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def SLR_Actuals(df, dates_shift: list, cols: list = ['Dates', 'Discipline', 'Activity', 'Location', 'Destination', 'Shift', 'Material', 'Quantity', 'Unit']):\n",
        "  '''\n",
        "  Description:\n",
        "    Export data table to an excel file containing the data entries corresponding to the SLR Tracker format in the Actuals spreadsheet tab\n",
        "\n",
        "  Arguments:\n",
        "    df: The cleaned dataframe\n",
        "    dates_shift: The dates for the weekly plan with the shift\n",
        "    cols: List of columns in final table\n",
        "\n",
        "  Outputs:\n",
        "    None\n",
        "\n",
        "  '''\n",
        "  df = df_in.copy()\n",
        "  del df_in\n",
        "\n",
        "  # Filter data table to only containing entries under Actual row\n",
        "  df_act = df[ df['Actual/Planned'] == 'Actual' ]\n",
        "  df_act = df_act.replace(0, np.nan, regex = True) # Replace 0's with NaN\n",
        "\n",
        "  # Create the data table to be exported as excel file\n",
        "  df_act_slr = pd.DataFrame(columns = cols)\n",
        "\n",
        "  for date_shift in dates_shift:\n",
        "    # Get all none NaN entries for a certain date and store temporarily\n",
        "    df_act.loc[:, date_shift] = df_act.loc[:, date_shift].apply(lambda w: np.nan if w == ' ' else w) # Replace manually entered space with NaN\n",
        "    temp = df_act[ (~df_act[date_shift].isna()) ]\n",
        "    df_temp = pd.DataFrame(columns = cols) # Temporary data table to be appended to master one\n",
        "    date, shift = date_shift.split()[0], date_shift.split()[1] # Separate dates and shift\n",
        "\n",
        "    # Fill Quantity column with amount\n",
        "    df_temp['Quantity'] = temp[date_shift].astype('float', errors = 'ignore')\n",
        "\n",
        "    # Add entries in the three columns to data table\n",
        "    df_temp[['Discipline', 'Activity', 'Location']] = temp[['Discipline', 'Activity', 'Location']]\n",
        "\n",
        "    # Enter TLO or Surface in Destination column depending on what's labelled in Activity column\n",
        "    df_temp['Destination'] = df_temp['Destination'].where(~df_temp['Activity'].str.contains('TLO', case = False), 'TLO')\n",
        "    df_temp['Destination'] = df_temp['Destination'].where(~df_temp['Activity'].str.contains('SURFACE', case = False), 'Surface')\n",
        "\n",
        "    # Update the Activity column to Trucking for waste haulages\n",
        "    df_temp['Activity'] = df_temp['Activity'].where(~df_temp['Activity'].isin(['WASTE TO TLO', 'WASTE TO SURFACE']), 'TRUCKING')\n",
        "\n",
        "    # Fill the Date column with the correct date\n",
        "    df_temp['Dates'].fillna(value = dt.strptime(date, '%Y-%m-%d'), inplace = True)\n",
        "\n",
        "    # Fill the Shift column with the correct shift\n",
        "    df_temp['Shift'] = shift\n",
        "\n",
        "    # Fill Material column with the correct material type\n",
        "    df_temp['Material'].fillna(value = 'ORE', inplace = True)\n",
        "    df_temp['Material'] = df_temp['Material'].where(~df_temp['Activity'].str.contains('WASTE'), 'WASTE')\n",
        "    df_temp['Material'] = df_temp['Material'].where(~df_temp['Discipline'].str.contains('BACKFILL'), 'BACKFILL')\n",
        "\n",
        "    # Fill Unit column with correct units\n",
        "    df_temp['Unit'].fillna(value = 't', inplace = True)\n",
        "    df_temp['Unit'] = df_temp['Unit'].where(~df_temp['Activity'].str.contains('DRILLING|DEVELOPMENT', regex = True), 'm')\n",
        "\n",
        "    # Append temporary table to master table\n",
        "    df_act_slr = pd.concat([df_act_slr, df_temp])\n",
        "    print('Added {} data to Actuals table'.format(date_shift))\n",
        "\n",
        "\n",
        "  # Excel file name\n",
        "  year, week = dt.strptime(dates_shift[0].split()[0], '%Y-%m-%d').isocalendar()[0], calweek.weeknum(dt.strptime(dates_shift[0].split()[0], '%Y-%m-%d')) # Based on starting date of the week\n",
        "  fname = 'Weekly{}_{}_actuals.xlsx'.format(year, week)\n",
        "\n",
        "  # Export to excel file (May require downloading)\n",
        "  if dpath is not None:\n",
        "    fname = os.path.join(dpath, fname)\n",
        "  with pd.ExcelWriter(fname, date_format = 'yyyy-mm-dd', datetime_format = 'yyyy-mm-dd', engine_kwargs = {'options': {'strings_to_numbers': True}}) as writer:\n",
        "    df_act_slr.to_excel(excel_writer = writer, index = False)\n",
        "  print('Actuals Export Completed for {}\\n'.format(fname))\n",
        "\n",
        "  # Dataframe information\n",
        "  # df_act_slr\n",
        "\n",
        "  return"
      ],
      "metadata": {
        "id": "OETWkjx9sZaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def SLR_Planned(df, dates_shift, cols: list = ['Dates', 'Plan', 'Activity', 'Discipline', 'Location', 'Shift', 'Material', 'Quantity', 'Unit']):\n",
        "  '''\n",
        "  Description:\n",
        "    Export data table to an excel file containing the data entries corresponding to the SLR Tracker format in the Planning spreadsheet tab\n",
        "\n",
        "  Arguments:\n",
        "    df: The cleaned dataframe\n",
        "    dates_shift: The dates for the weekly plan with the shift\n",
        "    cols: List of columns in final table\n",
        "\n",
        "  Outputs:\n",
        "    None\n",
        "\n",
        "  '''\n",
        "  df = df_in.copy()\n",
        "  del df_in\n",
        "\n",
        "  # Filter data table to only containing entries under Plan row\n",
        "  df_plan = df[ df['Actual/Planned'] == 'Plan' ]\n",
        "  df_plan = df_plan.replace(0, np.nan, regex = True) # Replace 0's with NaN\n",
        "\n",
        "  # Create the data table to be exported as excel file\n",
        "  df_plan_slr = pd.DataFrame(columns = cols)\n",
        "\n",
        "  for date_shift in dates_shift:\n",
        "    # Get all none NaN entries for a certain date and store temporarily\n",
        "    df_plan.loc[:, date_shift] = df_plan.loc[:, date_shift].apply(lambda w: np.nan if w == ' ' else w) # Replace manually entered space with NaN\n",
        "    temp = df_plan[ (~df_plan[date_shift].isna()) ]\n",
        "    df_temp = pd.DataFrame(columns = cols) # Temporary data table to be appended to master one\n",
        "    date, shift = date_shift.split()[0], date_shift.split()[1] # Separate dates and shift\n",
        "\n",
        "    # Fill Quantity column with amount\n",
        "    df_temp['Quantity'] = temp[date_shift].astype('float', errors = 'ignore')\n",
        "\n",
        "    # Add entries in the three columns to data table\n",
        "    df_temp[['Discipline', 'Activity', 'Location']] = temp[['Discipline', 'Activity', 'Location']]\n",
        "\n",
        "    # Update the Activity column to Trucking for waste haulages\n",
        "    df_temp['Activity'] = df_temp['Activity'].where(~df_temp['Activity'].isin(['WASTE TO TLO', 'WASTE TO SURFACE']), 'TRUCKING')\n",
        "\n",
        "    # Fill the Date column with the correct date\n",
        "    df_temp['Dates'].fillna(value = dt.strptime(date, '%Y-%m-%d'), inplace = True)\n",
        "\n",
        "    # Fill the Shift column with the correct shift\n",
        "    df_temp['Shift'] = shift\n",
        "\n",
        "    # Fill Material column with the correct material type\n",
        "    df_temp['Material'].fillna(value = 'ORE', inplace = True)\n",
        "    df_temp['Material'] = df_temp['Material'].where(~df_temp['Activity'].str.contains('WASTE'), 'WASTE')\n",
        "    df_temp['Material'] = df_temp['Material'].where(~df_temp['Discipline'].str.contains('BACKFILL'), 'BACKFILL')\n",
        "\n",
        "    # Fill Unit column with correct units\n",
        "    df_temp['Unit'].fillna(value = 't', inplace = True)\n",
        "    df_temp['Unit'] = df_temp['Unit'].where(~df_temp['Activity'].str.contains('DRILLING|DEVELOPMENT', regex = True), 'm')\n",
        "\n",
        "    # Append temporary table to master table\n",
        "    df_plan_slr = pd.concat([df_plan_slr, df_temp])\n",
        "    print('Added {} data to Planned table'.format(date_shift))\n",
        "\n",
        "\n",
        "  # Excel file name\n",
        "  year, week = dt.strptime(dates_shift[0].split()[0], '%Y-%m-%d').isocalendar()[0], calweek.weeknum(dt.strptime(dates_shift[0].split()[0], '%Y-%m-%d')) # Based on starting date of the week\n",
        "  fname = 'Weekly{}_{}_planned.xlsx'.format(year, week)\n",
        "\n",
        "  # Add the Plan name to Plan Column\n",
        "  df_plan_slr['Plan'] = 'Weekly{}_{}'.format(year, week)\n",
        "\n",
        "  # Export to excel file (May require downloading)\n",
        "  if dpath is not None:\n",
        "    fname = os.path.join(dpath, fname)\n",
        "  with pd.ExcelWriter(fname, date_format = 'yyyy-mm-dd', datetime_format = 'yyyy-mm-dd', engine_kwargs = {'options': {'strings_to_numbers': True}}) as writer:\n",
        "    df_plan_slr.to_excel(excel_writer = writer, index = False)\n",
        "  print('Planned Export Complete for {}\\n'.format(fname))\n",
        "\n",
        "  # Dataframe information\n",
        "  # df_act_plan\n",
        "\n",
        "  return"
      ],
      "metadata": {
        "id": "YeX15tQ53j15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "########################################################### PLEASE FOLLOW PROMPT FROM OUTPUT SCREEN BELOW ###########################################################\n",
        "print('Please upload a UTF-8 csv file only')\n",
        "wkly_tracker_csv = files.upload()\n",
        "\n",
        "df_cleaned, dates_and_shifts = data_cleaning(col_adjust(csv_import(csv_file = wkly_tracker_csv)))\n",
        "SLR_Actuals(df_cleaned, dates_and_shifts)\n",
        "SLR_Planned(df_cleaned, dates_and_shifts)\n",
        "\n",
        "print('\\nAll Completed. Files can be downloaded by clicking on \"Files\" icon on the left tab')\n",
        "print('The resulting tables in the created excel files can be copied and pasted directly to the SLR Tracker')\n",
        "print('Rerun this cell or from the start to continue processing data')\n",
        "print('Code written by Peng Yang (August, 2023)\\n')"
      ],
      "metadata": {
        "id": "5EufpSM1n5ta"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}